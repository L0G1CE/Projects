# Software Engineering Project â€“ ASL Gesture Recognition & Translation System

## ğŸ“Œ Overview
This project is a **Software Engineering capstone/research project** that develops an **American Sign Language (ASL) gesture recognition and translation system**.  

It combines:  
- **Dynamic gesture recognition** using an **LSTM model** trained on recorded ASL sequences.  
- **Natural language translation** using a **fine-tuned T5 model** to generate grammatically correct sentences from recognized gestures.  

The system serves as both:  
- A **research prototype** to explore multi-model ASL translation pipelines.  
- A **practical application** allowing end-users to record gestures, train models, and translate ASL into text.

---

## ğŸš€ Features
- ğŸ¥ **Gesture Recording** â€“ Capture ASL gesture sequences for dataset creation.  
- ğŸ§  **LSTM Training** â€“ Train a recurrent neural network (`asl_dynamic_lstm.h5`) on recorded gesture sequences.  
- ğŸ“ **LSTM Evaluation** â€“ Assess accuracy and performance of gesture recognition (`lstm_evaluate.py`).  
- ğŸ”¤ **T5 Fine-Tuning** â€“ Adapt a pre-trained T5 transformer for ASL-to-English translation (`fine_tune_t5.py`).  
- ğŸ“Š **T5 Evaluation** â€“ Measure quality of generated translations (`t5_evaluate.py`).  
- ğŸ”— **Prediction Pipeline** â€“ Convert recorded gestures â†’ recognized tokens â†’ natural sentences (`predict_to_sentence.py`).  
- ğŸ“‚ **Pre-trained Models & Checkpoints** â€“ Includes fine-tuned T5 model artifacts and checkpoints.  

---

## ğŸ“‚ Project Structure
```
SE_project/
â”‚â”€â”€ data/                       # Dataset folder (ASL recordings, sequences)
â”‚â”€â”€ t5_finetuned_asl/           # Fine-tuned T5 model artifacts
â”‚   â”œâ”€â”€ checkpoint-100/         # Saved model checkpoint
â”‚   â”œâ”€â”€ final_model/            # Final trained T5 model
â”‚   â”œâ”€â”€ runs/                   # Training logs/runs
â”‚   â”œâ”€â”€ config / tokenizer / added_tokens ...  
â”‚
â”‚â”€â”€ asl_data.xlsx               # Processed ASL dataset
â”‚â”€â”€ asl_dynamic_lstm.h5         # Trained LSTM model
â”‚â”€â”€ label_encoder.npy           # Encoded labels for gestures
â”‚
â”‚â”€â”€ record_gesture_sequences.py # Capture gesture sequences
â”‚â”€â”€ train_lstm.py               # Train LSTM on ASL data
â”‚â”€â”€ lstm_evaluate.py            # Evaluate LSTM model
â”‚â”€â”€ fine_tune_t5.py             # Fine-tune T5 for translation
â”‚â”€â”€ t5_evaluate.py              # Evaluate T5 translation
â”‚â”€â”€ predict_to_sentence.py      # End-to-end prediction pipeline
```

---

## ğŸ› ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/SE_project.git
cd SE_project
```

### 2ï¸âƒ£ Create a Virtual Environment
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

*(If no `requirements.txt` is provided, you will need: `tensorflow/keras`, `torch`, `transformers`, `numpy`, `pandas`, `scikit-learn`, `opencv-python`, `matplotlib`, etc.)*

---

## ğŸ“¸ Usage Workflow

### Step 1: Record Gestures
```bash
python record_gesture_sequences.py
```
- Collect gesture sequences for training (saved in `data/`).

### Step 2: Train the LSTM
```bash
python train_lstm.py
```
- Trains the LSTM model (`asl_dynamic_lstm.h5`).

### Step 3: Evaluate the LSTM
```bash
python lstm_evaluate.py
```
- Tests accuracy of gesture recognition.

### Step 4: Fine-Tune T5
```bash
python fine_tune_t5.py
```
- Fine-tunes a pre-trained T5 model on recognized ASL sequences for translation.

### Step 5: Evaluate T5
```bash
python t5_evaluate.py
```
- Measures translation performance (BLEU, ROUGE, etc.).

### Step 6: Predict Sentences
```bash
python predict_to_sentence.py
```
- Runs the end-to-end pipeline:  
  **ASL gesture â†’ LSTM recognition â†’ T5 translation â†’ English sentence.**

---

## âš™ï¸ Tech Stack
- **Deep Learning:** TensorFlow/Keras (LSTM), PyTorch + Hugging Face Transformers (T5).  
- **Data Handling:** NumPy, Pandas.  
- **Visualization:** Matplotlib.  
- **Preprocessing:** OpenCV (gesture recording).  

---

## ğŸ“Œ Future Enhancements
- ğŸ¤ Add real-time **webcam-based recognition** (continuous ASL to text).  
- ğŸ”Š Integrate **text-to-speech** for voice output.  
- ğŸ“± Deploy as a **mobile app** (using TensorFlow Lite or ONNX).  
- â˜ï¸ Cloud-based training + hosting for scalability.  

---

## ğŸ‘¨â€ğŸ’» Contributors
- Jericho Lampano (Lead Developer, Researcher)  
- Jesse Rey Isidro (Model Trainer, researcher)
- John Lloyd Apawan (Model Evaluator, researcher)
- John Paul Caya (Data Gatherer, researcher)